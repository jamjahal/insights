{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actionable Insights: Extracting Constructive Material from Customer Reviews\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xVZW6EDJq74"
   },
   "source": [
    "# PART 2: EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for modeling, the text must be preprocessed.  I will label the data using a custom component within SpaCy to find sentences containing actionable words, as determined from a list.  The list of actionable words is by no means exhaustive, but should serve as an aid in labeling the data, and with the help of semantic simlarity search (another SpaCy method) can be extended as appropriate.  This labeling process was augmented using the MonkeyLearn API and verified manually. \n",
    "\n",
    "To distill actionable insights, the reviews are broken by sentence.  However the context of the insights would be lost if broken down further to the individual word level, so instead we will analyze tokenized, and vectorized sentences for the models.  SpaCy will assign each token a 300 dimension vector, and each sentence will also be assigned a 300 dimentional vector based on the tokens components.  \n",
    "\n",
    "Finally the sentences were saved as dataframes for use in the following modeling notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Load in G2 Data](#Load-in-G2-Data)\n",
    "- [Search for Actionable Words](#Search-for-Actionable-Words)\n",
    "- [Extracting Sentences](#Extracting-Sentences)\n",
    "- [Checking the Target Distribution](#Checking-the-Target-Distribution)\n",
    "- [Adding Labels](#Adding-Labels)\n",
    "- [Combining the labels](#Combining-the-Labels)\n",
    "- [Saving Dataframes](#Saving-Dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 462,
     "status": "error",
     "timestamp": 1574804852006,
     "user": {
      "displayName": "Allan Hall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDSarmADkvJn6QPlafnX7h4FXzl02lpx40x7tgBnZI=s64",
      "userId": "06614119083687436954"
     },
     "user_tz": 480
    },
    "id": "XLaFBCG2Jq77",
    "outputId": "07f77f8f-b74e-4d59-9658-878cff744d42"
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "\n",
    "# Plot formatting\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n",
    "\n",
    "# NLP libraries\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from spacy.tokens import Span, Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "import scattertext as st\n",
    "\n",
    "# Modeling and Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "\n",
    "# Hide excessive future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##SQL libraries for the Google Play Store Review dataset\n",
    "# from sqlalchemy import create_engine\n",
    "# import psycopg2\n",
    "# import sqlalchemy\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "# import mysql\n",
    "# import mysql.connector\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZwq2d-cJq7_"
   },
   "outputs": [],
   "source": [
    "# Load in the SpaCy NLP Library\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WV1U2CBJq8f"
   },
   "source": [
    "## Load in G2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCY_LRI2Jq8g"
   },
   "outputs": [],
   "source": [
    "G2=pd.read_json('../Data/G2_hubspot_data.jl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbyGOIyKJq8i"
   },
   "source": [
    "Dropping 106 empty review rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMj9JMSfJq8j"
   },
   "outputs": [],
   "source": [
    "G2=G2[G2['reviews']!={}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2336,
     "status": "ok",
     "timestamp": 1574730169163,
     "user": {
      "displayName": "Allan Hall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDSarmADkvJn6QPlafnX7h4FXzl02lpx40x7tgBnZI=s64",
      "userId": "06614119083687436954"
     },
     "user_tz": 480
    },
    "id": "Zn3o9IoSJq8l",
    "outputId": "f6676865-f353-4926-e7ff-010674f10321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['What do you like best?', 'What do you dislike?', 'Recommendations to others considering the product:', 'What problems are you solving with the product?  What benefits have you realized?'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2.reviews[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMJBEJJ6Jq8n"
   },
   "outputs": [],
   "source": [
    "G2['pro'] =[G2.reviews[x]['What do you like best?'] for x in G2.index]\n",
    "G2['con'] =[G2.reviews[x]['What do you dislike?'] for x in G2.index]\n",
    "# Recommendation question not always asked, so including a conditional for this column\n",
    "G2['user_recs'] =[G2.reviews[x]['Recommendations to others considering the product:'] if 'Recommendations to others considering the product:'in G2.reviews[x].keys() else \"\" for x in G2.index]\n",
    "G2['value'] =[G2.reviews[x]['What problems are you solving with the product?  What benefits have you realized?'] for x in G2.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for Actionable Words\n",
    "Search for actionable words and search for similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most_similar function below allows me to find similar words to expand the list of actionable insight keywords to search for.  This will help with the labeling process.  It uses SpaCy's semantic similarity method based on the [GloVe](https://nlp.stanford.edu/projects/glove/) word vector algorithm developed at Stanford.  This list is by no means exhaustive but should prove useful in aiding the labeling process.  The words were decided upon after reviewing a sample of reviews manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    word=nlp.vocab[word]\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [word.lower_ for word in by_similarity[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wish',\n",
       " 'want',\n",
       " 'hope',\n",
       " 'wishing',\n",
       " 'you',\n",
       " 'know',\n",
       " 'forget',\n",
       " 'wished',\n",
       " 'let',\n",
       " 'glad']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter search term here to find 10 similar terms\n",
    "most_similar('wish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3yHCx_HJq8s"
   },
   "outputs": [],
   "source": [
    "# Actionable insight words\n",
    "words = 'able ability dislikes really favor ability should add glitch wish want hope glad maybe fix bug improve price better feature frustrating useful would hard consider try improve more issue suggest please request quality error flaw trouble could update break workaround solution want error issue hate love favorite'\n",
    "actionable_words=nlp(words)\n",
    "actionable_words=[token.lemma_ for token in actionable_words]    # Using Lemma's to capture more instances\n",
    "action_pattern = [{\"LEMMA\": {\"IN\": actionable_words}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am setting a [custom method](https://spacy.io/usage/processing-pipelines#custom-components-simple) to tag sentences containing the 'actionable' words from above 'has_action'.  Then I'm separating the reviews based on the question they are answering.  This can be used for context when looking at the sentences once tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idkuSDf1Jq8u"
   },
   "outputs": [],
   "source": [
    "# Setting a custom extention\n",
    "is_action_getter = lambda token: token.lemma_ in actionable_words\n",
    "has_action_getter = lambda obj: any([t.lemma_ in actionable_words for t in obj])\n",
    "\n",
    "Token.set_extension(\"is_action\", getter=is_action_getter, force=True)\n",
    "Doc.set_extension(\"has_action\", getter=has_action_getter, force=True)\n",
    "Span.set_extension(\"has_action\", getter=has_action_getter, force=True)\n",
    "\n",
    "# Setting up NLP for each question's response\n",
    "ProDocs = list(nlp.pipe(G2['pro']))\n",
    "ConDocs = list(nlp.pipe(G2['con']))\n",
    "RecsDocs = list(nlp.pipe(G2['user_recs']))\n",
    "ValueDocs = list(nlp.pipe(G2['value']))\n",
    "\n",
    "# Creating a Dataframe Column for each question's response within the reviews\n",
    "G2[\"pro_tokens\"] = ProDocs\n",
    "G2[\"con_tokens\"] = ConDocs\n",
    "G2[\"recs_tokens\"] = RecsDocs\n",
    "G2[\"value_tokens\"] = ValueDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuJ5ad2mJq9K"
   },
   "source": [
    "I will be able to extract a great deal of context from the vectorized column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oldOZ9cJq9Q"
   },
   "source": [
    "## Extracting Sentences\n",
    "### Seperating out sentences into a seperate dataframe and removing stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrwlmP7JJq9S"
   },
   "outputs": [],
   "source": [
    "target=[]\n",
    "sent_list=[]\n",
    "tokens=[]\n",
    "vectors=[]\n",
    "G2_index=[]\n",
    "column = []\n",
    "for i in G2.index:\n",
    "    for col in ['pro_tokens','con_tokens','recs_tokens','value_tokens']:\n",
    "        for sent in G2[col][i].sents:\n",
    "            column.append(col)\n",
    "            G2_index.append(i)\n",
    "            sent_list.append(sent)\n",
    "            \n",
    "            # Lemmatizing, removing stop words and punctuation\n",
    "            sent = [token.lemma_ for token in sent if token.is_stop==False and token.is_punct ==False and token.lemma_ !='-PRON-']\n",
    "            sent = ' '.join(sent)\n",
    "            sent = nlp.make_doc(sent)   \n",
    "            tokens.append(sent)\n",
    "            vectors.append(sent.vector)\n",
    "            if sent._.has_action:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "\n",
    "# Creating Dataframe of sentencized reviews\n",
    "G2_sent=pd.DataFrame({'from_row': G2_index,\n",
    "                      'from_col': column,\n",
    "                      'sentence': sent_list,\n",
    "                      'tokens'  : tokens,\n",
    "                      'vector'  : vectors,\n",
    "                      'is_actionable': target,\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.715135\n",
       "1    0.284865\n",
       "Name: is_actionable, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2_sent['is_actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Labels\n",
    "I manually labelled 400 of the sentences and will use a [Monkeylearn](https://monkeylearn.com/) api trained on those 400 sentences to suggest tags for the rest, which I will validate manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled=pd.read_csv('../Data/G2_Review_Categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text             0\n",
       "Tag           4960\n",
       "Confidence    5352\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2_labeled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values removed successfully from \"Tags\"\n"
     ]
    }
   ],
   "source": [
    "# Filling null values in tags.\n",
    "G2_labeled['Tag'].fillna('none', inplace=True)\n",
    "if G2_labeled['Tag'].isnull().sum() ==0:\n",
    "    print('null values removed successfully from \"Tags\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled = pd.concat([G2_labeled.drop('Confidence', axis=1), G2_labeled['Tag'].str.get_dummies(sep=':')], axis=1)\n",
    "# Reviewing the updated target distribution\n",
    "G2_sent['is_actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting spacy docs and spans back into strings to allow or dataset merge\n",
    "# G2_sent['sentence']=G2_sent['sentence'].map(lambda x: x.text)\n",
    "# G2_labeled['Text']=G2_labeled['Text'].map(lambda x:x.text)\n",
    "# Merge datasets\n",
    "G2_labeled=pd.merge(left=G2_sent, right=G2_labeled, how='left',left_on='sentence',right_on='Text', right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_row</th>\n",
       "      <th>from_col</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector</th>\n",
       "      <th>is_actionable</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Actionable</th>\n",
       "      <th>No tags</th>\n",
       "      <th>UX</th>\n",
       "      <th>alternatives</th>\n",
       "      <th>bugs</th>\n",
       "      <th>features</th>\n",
       "      <th>integrations</th>\n",
       "      <th>none</th>\n",
       "      <th>price</th>\n",
       "      <th>support</th>\n",
       "      <th>updates</th>\n",
       "      <th>wishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pro_tokens</td>\n",
       "      <td>I have used HubSpot in few companies from star...</td>\n",
       "      <td>(HubSpot, company, start, up, medium, sized, o...</td>\n",
       "      <td>[0.07115748, 0.19749413, -0.06404737, -0.10758...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have used HubSpot in few companies from star...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pro_tokens</td>\n",
       "      <td>And based on my experirnce I think the key hig...</td>\n",
       "      <td>(base, experirnce, think, key, highlight, user...</td>\n",
       "      <td>[0.06888762, 0.08969499, -0.06350486, 0.048314...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>con_tokens</td>\n",
       "      <td>\\r</td>\n",
       "      <td>(\\r)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>recs_tokens</td>\n",
       "      <td>Everything is very well connected, and you can...</td>\n",
       "      <td>(connected, develop, tool, have, user, mind)</td>\n",
       "      <td>[0.08063999, 0.120635055, -0.15673333, -0.0605...</td>\n",
       "      <td>0</td>\n",
       "      <td>Everything is very well connected, and you can...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>recs_tokens</td>\n",
       "      <td>I have used or tested other marketing automati...</td>\n",
       "      <td>(test, marketing, automation, tool, HubSpot, e...</td>\n",
       "      <td>[-0.19968951, 0.16349284, -0.07618576, 0.02723...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from_row     from_col                                           sentence  \\\n",
       "0         1   pro_tokens  I have used HubSpot in few companies from star...   \n",
       "1         1   pro_tokens  And based on my experirnce I think the key hig...   \n",
       "2         1   con_tokens                                                 \\r   \n",
       "3         1  recs_tokens  Everything is very well connected, and you can...   \n",
       "4         1  recs_tokens  I have used or tested other marketing automati...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  (HubSpot, company, start, up, medium, sized, o...   \n",
       "1  (base, experirnce, think, key, highlight, user...   \n",
       "2                                               (\\r)   \n",
       "3       (connected, develop, tool, have, user, mind)   \n",
       "4  (test, marketing, automation, tool, HubSpot, e...   \n",
       "\n",
       "                                              vector  is_actionable  \\\n",
       "0  [0.07115748, 0.19749413, -0.06404737, -0.10758...              1   \n",
       "1  [0.06888762, 0.08969499, -0.06350486, 0.048314...              0   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              0   \n",
       "3  [0.08063999, 0.120635055, -0.15673333, -0.0605...              0   \n",
       "4  [-0.19968951, 0.16349284, -0.07618576, 0.02723...              0   \n",
       "\n",
       "                                                Text   Tag  Actionable  \\\n",
       "0  I have used HubSpot in few companies from star...  none         0.0   \n",
       "1                                                NaN   NaN         NaN   \n",
       "2                                                NaN   NaN         NaN   \n",
       "3  Everything is very well connected, and you can...  none         0.0   \n",
       "4                                                NaN   NaN         NaN   \n",
       "\n",
       "   No tags   UX  alternatives  bugs  features  integrations  none  price  \\\n",
       "0      0.0  0.0           0.0   0.0       0.0           0.0   1.0    0.0   \n",
       "1      NaN  NaN           NaN   NaN       NaN           NaN   NaN    NaN   \n",
       "2      NaN  NaN           NaN   NaN       NaN           NaN   NaN    NaN   \n",
       "3      0.0  0.0           0.0   0.0       0.0           0.0   1.0    0.0   \n",
       "4      NaN  NaN           NaN   NaN       NaN           NaN   NaN    NaN   \n",
       "\n",
       "   support  updates  wishes  \n",
       "0      0.0      0.0     0.0  \n",
       "1      NaN      NaN     NaN  \n",
       "2      NaN      NaN     NaN  \n",
       "3      0.0      0.0     0.0  \n",
       "4      NaN      NaN     NaN  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2_labeled.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    5117\n",
       "1.0     220\n",
       "Name: Actionable, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2_labeled['Actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls have been successfully filled\n"
     ]
    }
   ],
   "source": [
    "# Isolate labeled training set which can be used for sub categorization training of the actionable sentences\n",
    "G2_train = G2_labeled.loc[G2_labeled['Text'].isnull()==False]\n",
    "\n",
    "# Fill NaN's\n",
    "G2_labeled.fillna(0, inplace=True)\n",
    "if G2_labeled.isnull().sum().sum()==0:\n",
    "    print('Nulls have been successfully filled')\n",
    "else:\n",
    "    print(f'there are still null values:', G2_labeled.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-vectorizing the text\n",
    "G2_labeled['Text']=list(nlp.pipe(labeled_g2['Text']))\n",
    "\n",
    "# Combining labels fo actionable sentences\n",
    "G2_labeled['is_actionable'].loc[G2_labeled['Actionable']==1]=1\n",
    "G2_labeled['Actionable'] = G2_labeled['is_actionable']\n",
    "G2_labeled.drop('is_actionable', inplace=True, axis=1)\n",
    "G2_labeled['Actionable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_train.to_pickle('../Data/G2_train.pkl')\n",
    "G2_labeled.to_pickle('../Data/G2_labeled.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "X1bL0-9xJq8C",
    "dTCwYVBMJq8S",
    "0zL84cCqJq9j",
    "Q5HlZ79TJq9o",
    "rwAhBD_YJq-V",
    "9hcTkIcfJq-e",
    "uoaFLpd2Jq-s"
   ],
   "name": "2_EDA_and_Data_Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
