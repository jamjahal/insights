{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actionable Insights: Extracting Constructive Material from Customer Reviews\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xVZW6EDJq74"
   },
   "source": [
    "# PART 2: EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for modeling, the text must be preprocessed.  I will label the data using a custom component within SpaCy to find sentences containing actionable words, as determined from a list.  The list of actionable words is by no means exhaustive, but should serve as an aid in labeling the data, and with the help of semantic simlarity search (another SpaCy method) can be extended as appropriate.  This labeling process was augmented using the MonkeyLearn API and verified manually. \n",
    "\n",
    "To distill actionable insights, the reviews are broken by sentence.  However the context of the insights would be lost if broken down further to the individual word level, so instead we will analyze tokenized, and vectorized sentences for the models.  SpaCy will assign each token a 300 dimension vector, and each sentence will also be assigned a 300 dimentional vector based on the tokens components.  \n",
    "\n",
    "Finally the sentences were saved as dataframes for use in the following modeling notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Load in G2 Data](#Load-in-G2-Data)\n",
    "- [Search for Actionable Words](#Search-for-Actionable-Words)\n",
    "- [Extracting Sentences](#Extracting-Sentences)\n",
    "- [Checking the Target Distribution](#Checking-the-Target-Distribution)\n",
    "- [Adding Labels](#Adding-Labels)\n",
    "- [Combining the labels](#Combining-the-Labels)\n",
    "- [Saving Dataframes](#Saving-Dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 462,
     "status": "error",
     "timestamp": 1574804852006,
     "user": {
      "displayName": "Allan Hall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDSarmADkvJn6QPlafnX7h4FXzl02lpx40x7tgBnZI=s64",
      "userId": "06614119083687436954"
     },
     "user_tz": 480
    },
    "id": "XLaFBCG2Jq77",
    "outputId": "07f77f8f-b74e-4d59-9658-878cff744d42"
   },
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "\n",
    "# Plot formatting\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n",
    "\n",
    "# NLP libraries\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from spacy.tokens import Span, Doc, Token\n",
    "from spacy.matcher import Matcher\n",
    "import scattertext as st\n",
    "\n",
    "# Modeling and Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "\n",
    "# Hide excessive future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##SQL libraries for the Google Play Store Review dataset\n",
    "# from sqlalchemy import create_engine\n",
    "# import psycopg2\n",
    "# import sqlalchemy\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "# import mysql\n",
    "# import mysql.connector\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZwq2d-cJq7_"
   },
   "outputs": [],
   "source": [
    "# Load in the SpaCy NLP Library\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WV1U2CBJq8f"
   },
   "source": [
    "## Load in G2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCY_LRI2Jq8g"
   },
   "outputs": [],
   "source": [
    "G2=pd.read_json('../Data/G2_hubspot_data.jl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbyGOIyKJq8i"
   },
   "source": [
    "Dropping 106 empty review rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMj9JMSfJq8j"
   },
   "outputs": [],
   "source": [
    "G2=G2[G2['reviews']!={}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2336,
     "status": "ok",
     "timestamp": 1574730169163,
     "user": {
      "displayName": "Allan Hall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDSarmADkvJn6QPlafnX7h4FXzl02lpx40x7tgBnZI=s64",
      "userId": "06614119083687436954"
     },
     "user_tz": 480
    },
    "id": "Zn3o9IoSJq8l",
    "outputId": "f6676865-f353-4926-e7ff-010674f10321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['What do you like best?', 'What do you dislike?', 'Recommendations to others considering the product:', 'What problems are you solving with the product?  What benefits have you realized?'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2.reviews[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMJBEJJ6Jq8n"
   },
   "outputs": [],
   "source": [
    "G2['pro'] =[G2.reviews[x]['What do you like best?'] for x in G2.index]\n",
    "G2['con'] =[G2.reviews[x]['What do you dislike?'] for x in G2.index]\n",
    "# Recommendation question not always asked, so including a conditional for this column\n",
    "G2['user_recs'] =[G2.reviews[x]['Recommendations to others considering the product:'] if 'Recommendations to others considering the product:'in G2.reviews[x].keys() else \"\" for x in G2.index]\n",
    "G2['value'] =[G2.reviews[x]['What problems are you solving with the product?  What benefits have you realized?'] for x in G2.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for Actionable Words\n",
    "Search for actionable words and search for similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most_similar function below allows me to find similar words to expand the list of actionable insight keywords to search for.  This will help with the labeling process.  It uses SpaCy's semantic similarity method based on the [GloVe](https://nlp.stanford.edu/projects/glove/) word vector algorithm developed at Stanford.  This list is by no means exhaustive but should prove useful in aiding the labeling process.  The words were decided upon after reviewing a sample of reviews manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>company_size</th>\n",
       "      <th>role</th>\n",
       "      <th>date</th>\n",
       "      <th>reviews</th>\n",
       "      <th>pro</th>\n",
       "      <th>con</th>\n",
       "      <th>user_recs</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>[Matthew T]</td>\n",
       "      <td>[(11-50 employees)]</td>\n",
       "      <td>[Marketing Manager, Utilities]</td>\n",
       "      <td>[Jun 24, 2019]</td>\n",
       "      <td>{'What do you like best?': '• Continuous produ...</td>\n",
       "      <td>• Continuous product updates\\r</td>\n",
       "      <td>• Tons of education and support\\r</td>\n",
       "      <td>• Too many features to count, segmentation, tr...</td>\n",
       "      <td>• While there are a lot of features, it is sti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author         company_size                            role  \\\n",
       "250  [Matthew T]  [(11-50 employees)]  [Marketing Manager, Utilities]   \n",
       "\n",
       "               date                                            reviews  \\\n",
       "250  [Jun 24, 2019]  {'What do you like best?': '• Continuous produ...   \n",
       "\n",
       "                                pro                                con  \\\n",
       "250  • Continuous product updates\\r  • Tons of education and support\\r   \n",
       "\n",
       "                                             user_recs  \\\n",
       "250  • Too many features to count, segmentation, tr...   \n",
       "\n",
       "                                                 value  \n",
       "250  • While there are a lot of features, it is sti...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    word=nlp.vocab[word]\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [word.lower_ for word in by_similarity[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter search term here to find 10 similar terms\n",
    "most_similar('wish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3yHCx_HJq8s"
   },
   "outputs": [],
   "source": [
    "# Actionable insight words\n",
    "words = 'able ability dislikes really favor ability should add glitch wish want hope glad maybe fix bug improve price better feature frustrating useful would hard consider try improve more issue suggest please request quality error flaw trouble could update break workaround solution want error issue hate love favorite'\n",
    "actionable_words=nlp(words)\n",
    "actionable_words=[token.lemma_ for token in actionable_words]    # Using Lemma's to capture more instances\n",
    "action_pattern = [{\"LEMMA\": {\"IN\": actionable_words}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am setting a [custom method](https://spacy.io/usage/processing-pipelines#custom-components-simple) to tag sentences containing the 'actionable' words from above 'has_action'.  Then I'm separating the reviews based on the question they are answering.  This can be used for context when looking at the sentences once tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idkuSDf1Jq8u"
   },
   "outputs": [],
   "source": [
    "# Setting a custom extention\n",
    "is_action_getter = lambda token: token.lemma_ in actionable_words\n",
    "has_action_getter = lambda obj: any([t.lemma_ in actionable_words for t in obj])\n",
    "\n",
    "Token.set_extension(\"is_action\", getter=is_action_getter, force=True)\n",
    "Doc.set_extension(\"has_action\", getter=has_action_getter, force=True)\n",
    "Span.set_extension(\"has_action\", getter=has_action_getter, force=True)\n",
    "\n",
    "# Setting up NLP for each question's response\n",
    "ProDocs = list(nlp.pipe(G2['pro']))\n",
    "ConDocs = list(nlp.pipe(G2['con']))\n",
    "RecsDocs = list(nlp.pipe(G2['user_recs']))\n",
    "ValueDocs = list(nlp.pipe(G2['value']))\n",
    "\n",
    "# Creating a Dataframe Column for each question's response within the reviews\n",
    "G2[\"What do you like best?\"] = ProDocs\n",
    "G2[\"What do you dislike?\"] = ConDocs\n",
    "G2[\"Recommendations for others considering product:\"] = RecsDocs\n",
    "G2[\"What problems are you solving with the product?\"] = ValueDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuJ5ad2mJq9K"
   },
   "source": [
    "I will be able to extract a great deal of context from the vectorized column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oldOZ9cJq9Q"
   },
   "source": [
    "## Extracting Sentences\n",
    "### Seperating out sentences into a seperate dataframe and removing stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrwlmP7JJq9S"
   },
   "outputs": [],
   "source": [
    "target=[]\n",
    "sent_list=[]\n",
    "tokens=[]\n",
    "vectors=[]\n",
    "G2_index=[]\n",
    "column = []\n",
    "for i in G2.index:\n",
    "    for col in ['What do you like best?','What do you dislike?','Recommendations for others considering product:','What problems are you solving with the product?']:\n",
    "        for sent in G2[col][i].sents:\n",
    "            column.append(col)\n",
    "            G2_index.append(i)\n",
    "            sent_list.append(sent)\n",
    "            \n",
    "            # Lemmatizing, removing stop words and punctuation\n",
    "            sent = [token.lemma_ for token in sent if token.is_stop==False and token.is_punct ==False and token.lemma_ !='-PRON-']\n",
    "            sent = ' '.join(sent)\n",
    "            sent = nlp.make_doc(sent)   \n",
    "            tokens.append(sent)\n",
    "            vectors.append(sent.vector)\n",
    "            if sent._.has_action:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "\n",
    "\n",
    "# Creating Dataframe of sentencized reviews\n",
    "G2_sent=pd.DataFrame({'from_row': G2_index,\n",
    "                      'from_col': column,\n",
    "                      'sentence': sent_list,\n",
    "                      'tokens'  : tokens,\n",
    "                      'vector'  : vectors,\n",
    "                      'is_actionable': target,\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_sent['is_actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Labels\n",
    "I manually labelled 400 of the sentences and will use a [Monkeylearn](https://monkeylearn.com/) api trained on those 400 sentences to suggest tags for the rest, which I will validate manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled=pd.read_csv('../Data/G2_Review_Categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling null values in tags.\n",
    "G2_labeled['Tag'].fillna('none', inplace=True)\n",
    "if G2_labeled['Tag'].isnull().sum() ==0:\n",
    "    print('null values removed successfully from \"Tags\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled = pd.concat([G2_labeled.drop('Confidence', axis=1), G2_labeled['Tag'].str.get_dummies(sep=':')], axis=1)\n",
    "# Reviewing the updated target distribution\n",
    "G2_sent['is_actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(G2_sent['sentence'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting spacy docs and spans back into strings to allow or dataset merge\n",
    "# G2_sent['sentence']=G2_sent['sentence'].map(lambda x: x.text)\n",
    "# G2_labeled['Text']=G2_labeled['Text'].map(lambda x:x.text)\n",
    "# Merge datasets\n",
    "G2_labeled=pd.merge(left=G2_sent, right=G2_labeled, how='left',left_on='sentence',right_on='Text', right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_labeled['Actionable'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate labeled training set which can be used for sub categorization training of the actionable sentences\n",
    "G2_train = G2_labeled.loc[G2_labeled['Text'].isnull()==False]\n",
    "\n",
    "# Fill NaN's\n",
    "G2_labeled.fillna(0, inplace=True)\n",
    "G2_labeled['Text'] = G2_labeled['Text'].astype(str)\n",
    "if G2_labeled.isnull().sum().sum()==0:\n",
    "    print('Nulls have been successfully filled')\n",
    "else:\n",
    "    print(f'there are still null values:', G2_labeled.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm goint to return the vectorized tokens and text to strings to reduce the size of the saved DataFrame and work around some limitations of saving spacy docs in a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining labels for actionable sentences\n",
    "G2_labeled['is_actionable'].loc[G2_labeled['Actionable']==1]=1\n",
    "G2_labeled['Actionable'] = G2_labeled['is_actionable']\n",
    "G2_labeled.drop('is_actionable', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tokens to text in order to save dataframe\n",
    "for i in range(len(G2_labeled['sentence'])):\n",
    "    if type(G2_labeled['sentence'][i]) != str:    \n",
    "        G2_labeled['sentence'][i]=G2_labeled['sentence'][i].text\n",
    "        G2_labeled['tokens'][i]=G2_labeled['tokens'][i].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a label column with category names\n",
    "G2_labeled['label']= G2_labeled['Actionable'].map({1:'Actionable',0:'Not_Actionable'})\n",
    "G2_labeled['Actionable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_train.to_pickle('../Data/G2_train.pkl')\n",
    "G2_labeled.to_pickle('../Data/G2_labeled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "X1bL0-9xJq8C",
    "dTCwYVBMJq8S",
    "0zL84cCqJq9j",
    "Q5HlZ79TJq9o",
    "rwAhBD_YJq-V",
    "9hcTkIcfJq-e",
    "uoaFLpd2Jq-s"
   ],
   "name": "2_EDA_and_Data_Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
